{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G5833_A35_1747_R2 : HOLLIS# 013874235 , 10.3 , OpenStreetMap , 0 meters , WGS_1984_UTM_Zone_32N ( 32632 ), 2016_07_26 , The map is not drawn to scale. The point use for georeferencing were chosen to place the central focus of the map (the Fort at Antibes) in the correct location. Other features of the map are not geographically accurate.\n",
      "G5833_I3_1717_J3_COPYB : HOLLIS# 013945764 , 10.3 , OpenStreetMap; ESRI_World_Imagery , 501.967 meters , WGS_1984_UTM_Zone_31N ( 32631 ), 2016_07_26 , \n",
      "G5834_A12_1696_F4 : HOLLIS# 013867976 , 10.3 , OpenStreetMap; ESRI_World_Imagery , 44.3215 meters , WGS_1984_UTM_Zone_31N ( 32631 ), 2016_07_26 , Not all map features are drawn to scale, therefore, not all features are geographically accurate post-georeferencing.\n",
      "G5834_A287_1710_P5 : HOLLIS# 013871549 , 10.3 , OpenStreetMap; ESRI_World_Imagery , 19.1747 meters , WGS_1984_UTM_Zone_31N ( 32631 ), 2016_07_26 , Most of features on map are no longer visible, so georeferencing points were very limited.\n",
      "G5834_A425_1739_A6 : HOLLIS# 013874214 , 10.3 , OpenStreetMap; ESRI_World_Topographic_Map , 979.75 meters , WGS_1984_UTM_Zone_31N ( 32631 ), 2016_07_26 , \n",
      "G5834_A77_1696_F4 : HOLLIS# 013868075 , 10.3 , OpenStreetMap; ESRI_World_Imagery , 8.81895 meters , WGS_1984_UTM_Zone_32N ( 32632 ), 2016_07_26 , \n",
      "G5834_B3_1696_F4 : HOLLIS# 013868193 , 10.3 , ESRI_World_Imagery , 32.5562 meters , WGS_1984_UTM_Zone_30N ( 32630 ), 2016_07_26 , \n",
      "G5834_B45_1696_F4 : HOLLIS# 013867933 , 10.3 , OpenStreetMap; ESRI_World_Imagery , 5.17605 meters , WGS_1984_UTM_Zone_32N ( 32632 ), 2016_07_26 , \n",
      "G5834_B6_1696_F41 : HOLLIS# 013868188 , 10.3 , ESRI_World_Imagery , 27.9512 meters , WGS_1984_UTM_Zone_30N ( 32630 ), 2016_07_26 , None of the fort remains today, so georeferencing was based on street placement along old fort boundaries.\n",
      "G5834_B7_1696_F4_COPYA : HOLLIS# 013868213 , 10.3 , ESRI_World_Imagery , 63.801 meters , WGS_1984_UTM_Zone_30N ( 32630 ), 2016_07_26 , Most of the architectural map features no longer exist, so georeferencing is not precise.\n",
      "G5834_C3_1695_F4 : HOLLIS# 013867815 , 10.3 , ESRI_World_Imagery , 25.6851 meters , WGS_1984_UTM_Zone_31N ( 32631 ), 2016_07_27 , \n",
      "G5834_G7_1696_F4 : HOLLIS# 013868009 , 10.3 , ESRI_World_Imagery; ESRI_World_Topographic_Map , 45.2107 meters , WGS_1984_UTM_Zone_31N ( 32631 ), 2016_07_27 , Used a map published by the Musée dauphinois (http://www.musee-dauphinois.fr) in 'L'Isère en relief' in October 2012 to determine where the old city fortifications were located in relation to the modern street plan.\n",
      "G5834_L2_1696_F4 : HOLLIS# 013868199 , 10.3 , OpenStreetMap; ESRI_World_Imagery , 78.8134 meters , WGS_1984_UTM_Zone_30N ( 32630 ), 2016_07_27 , Most of features shown on map no longer exist, so georeferencing might not be precise for entire map.\n",
      "G5834_L846_1646_F4 : HOLLIS# 013867870 , 10.3 , OpenStreetMap; ESRI_World_Imagery , 9.16436 meters , WGS_1984_UTM_Zone_32N ( 32632 ), 2017_07_27 , \n",
      "G5833_S45_1682_B6 : HOLLIS# 014822272 , 10.5.1 , ESRI_World_Imagery , 0 meters , WGS_1984_World_Mercator ( 3395 ), 2018_02_27 , *NOTE* - map very inaccurate --> only 2 control points used to orient and resize map for relative geographic accuracy\n"
     ]
    }
   ],
   "source": [
    "with open('testCSV.csv') as csvFile:\n",
    "    reader = csv.DictReader(csvFile)\n",
    "    for row in reader:\n",
    "        print(row['OwnerSuppliedName'],': HOLLIS#', row['HOLLIS'],',', row['ArcGIS_version'],',', row['Georeferenced_to..'],',', row['RMS_error '], row['units'],',', row['Coord_Syst'],'(', row['EPSG_code'],'),', row['Date_Production'],',', row['notes'])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idinfo {}\n",
      "dataqual {}\n",
      "spdoinfo {}\n",
      "spref {}\n",
      "eainfo {}\n",
      "distinfo {}\n",
      "metainfo {}\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "with open('G5833_S45_1682_B6.xml') as f:\n",
    "    tree = ET.parse('G5833_S45_1682_B6.xml')\n",
    "    root = tree.getroot()\n",
    "\n",
    "    for child in root:\n",
    "        print(child.tag, child.attrib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://hgl.harvard.edu:8080/HGL/jsp/HGL.jsp?action=VColl&VCollName=rLayerID'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# index in to: /metadata/idinfo/citation/citeinfo/onlink - 'rLayerID'\n",
    "root[0][0][0][8].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import lxml.etree as ET\n",
    "#using lxml instead of xml preserved the comments\n",
    "\n",
    "#adding the encoding when the file is opened and written is needed to avoid a charmap error\n",
    "with open('G5833_S45_1682_B6.xml', encoding=\"utf8\") as f:\n",
    "    tree = ET.parse(f)\n",
    "    root = tree.getroot()\n",
    "   \n",
    "    # index in to: /metadata/idinfo/citation/citeinfo/onlink - 'rLayerID'\n",
    "    target1 = root[0][0][0][8]\n",
    "    # index in to: /metadata/idinfo/descript/abstract – rProjection \n",
    "    target2 = root[0][1][0]\n",
    "    #index in to: /metadata/idinfo/native - rArcVersion\n",
    "    target3 = root[0][9]\n",
    "    #index in to: /metadata/dataqual/posacc/horizpa/horizpar - rRMS rUnits\n",
    "    target4 = root[1][2][0][0]\n",
    "    #index in to: /metadata/dataqual/lineage/procstep/procdesc – rDataSources rProjection\n",
    "    target5 = root[1][3][1][0]\n",
    "    \n",
    "    \n",
    "    target1.text = target1.text.replace('http://hgl.harvard.edu:8080/HGL/jsp/HGL.jsp?action=VColl&VCollName=rLayerID','http://hgl.harvard.edu:8080/HGL/jsp/HGL.jsp?action=VColl&VCollName=G5833_S45_1682_B6')\n",
    "    #target2.text = target2.text.replace(''This layer is a georeferenced raster image of the historic paper map entitled: Ducatus Chablasius et Lacus Lemanus cum regionibus adjacentibus / It was published by: [Apud Haeredes Ioannes Blaeu], in 1682. Scale [ca. 1:220,000].\\n                    \\n                The image inside the map neatline is georeferenced to the surface of the earth and fit to the rProjection coordinate system. All map collar and inset information is also available as part of the raster image, including any inset maps, profiles, statistical tables, directories, text, illustrations, index maps, legends, or other information associated with the principal map. \\n                    \\n                This map shows features such as drainage, cities and other human settlements, territorial boundaries, shoreline features, and more.  rRelief  Includes also\\n                    \\n                This layer is part of a selection of digitally scanned and georeferenced historic maps from the Harvard Map Collection. These maps typically portray both natural and manmade features. The selection represents a range of originators, ground condition dates, scales, and map purposes.', )\n",
    "    target3.text = target3.text.replace('rArcVersion', '10.3')\n",
    "    #target4.text = target4.text.replace\n",
    "    #target5.text = target5.text.replace\n",
    "    \n",
    "#tree.write('output.xml', encoding=\"utf8\")\n",
    "# Adding the xml_declaration and method helped keep the header info at the top of the file.\n",
    "tree.write('TestOutput.xml', xml_declaration=True, method='xml', encoding=\"utf8\")    \n",
    "\n",
    "print ('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G5833_S45_1682_B6 XSLT updated\n",
      "MARC record downloaded from API\n",
      "XSLT transformation run\n",
      "G5833_S45_1682_B6 FGDC record edited\n",
      "1 FGDC record updated \n",
      "\n",
      "G5833_S45_1725_B6 XSLT updated\n",
      "MARC record downloaded from API\n",
      "XSLT transformation run\n",
      "G5833_S45_1725_B6 FGDC record edited\n",
      "2 FGDC records updated\n",
      "\n",
      "G6032_C6_1740_S4 XSLT updated\n",
      "MARC record downloaded from API\n",
      "XSLT transformation run\n",
      "G6032_C6_1740_S4 FGDC record edited\n",
      "3 FGDC records updated\n",
      "\n",
      "G6040_1800_W5 XSLT updated\n",
      "MARC record downloaded from API\n",
      "XSLT transformation run\n",
      "G6040_1800_W5 FGDC record edited\n",
      "4 FGDC records updated\n",
      "\n",
      "G6040_1815_S8 XSLT updated\n",
      "MARC record downloaded from API\n",
      "XSLT transformation run\n",
      "G6040_1815_S8 FGDC record edited\n",
      "5 FGDC records updated\n",
      "\n",
      "G6042_G4_1740_C3 XSLT updated\n",
      "MARC record downloaded from API\n",
      "XSLT transformation run\n",
      "G6042_G4_1740_C3 FGDC record edited\n",
      "6 FGDC records updated\n",
      "\n",
      "G6043_A2_1635_M4 XSLT updated\n",
      "MARC record downloaded from API\n",
      "XSLT transformation run\n",
      "G6043_A2_1635_M4 FGDC record edited\n",
      "7 FGDC records updated\n",
      "\n",
      "G6043_B3_1633_J3_COPY_A XSLT updated\n",
      "MARC record downloaded from API\n",
      "XSLT transformation run\n",
      "G6043_B3_1633_J3_COPY_A FGDC record edited\n",
      "8 FGDC records updated\n",
      "\n",
      "G6043_B3_1633_J3_COPY_B XSLT updated\n",
      "MARC record downloaded from API\n",
      "XSLT transformation run\n",
      "G6043_B3_1633_J3_COPY_B FGDC record edited\n",
      "9 FGDC records updated\n",
      "\n",
      "G6043_B3_1662_B5 XSLT updated\n",
      "MARC record downloaded from API\n",
      "XSLT transformation run\n",
      "G6043_B3_1662_B5 FGDC record edited\n",
      "10 FGDC records updated\n",
      "\n",
      "G6043_B4_1635_M4 XSLT updated\n",
      "MARC record downloaded from API\n",
      "XSLT transformation run\n",
      "G6043_B4_1635_M4 FGDC record edited\n",
      "11 FGDC records updated\n",
      "\n",
      "G6043_G7_1630_S6 XSLT updated\n",
      "MARC record downloaded from API\n",
      "XSLT transformation run\n",
      "G6043_G7_1630_S6 FGDC record edited\n",
      "12 FGDC records updated\n",
      "\n",
      "G6043_G7_1633_S6_COPY_A XSLT updated\n",
      "MARC record downloaded from API\n",
      "XSLT transformation run\n",
      "G6043_G7_1633_S6_COPY_A FGDC record edited\n",
      "13 FGDC records updated\n",
      "\n",
      "G6043_G7_1633_S6_COPY_B XSLT updated\n",
      "MARC record downloaded from API\n",
      "XSLT transformation run\n",
      "G6043_G7_1633_S6_COPY_B FGDC record edited\n",
      "14 FGDC records updated\n",
      "\n",
      "G6043_G7_1707_S6 XSLT updated\n",
      "MARC record downloaded from API\n",
      "XSLT transformation run\n",
      "G6043_G7_1707_S6 FGDC record edited\n",
      "15 FGDC records updated\n",
      "\n",
      "G6043_G7_1740_W3 XSLT updated\n",
      "MARC record downloaded from API\n",
      "XSLT transformation run\n",
      "G6043_G7_1740_W3 FGDC record edited\n",
      "16 FGDC records updated\n",
      "\n",
      "G6043_N4_1730_L5 XSLT updated\n",
      "MARC record downloaded from API\n",
      "XSLT transformation run\n",
      "G6043_N4_1730_L5 FGDC record edited\n",
      "17 FGDC records updated\n",
      "\n",
      "G6043_N4_1745_L5 XSLT updated\n",
      "MARC record downloaded from API\n",
      "XSLT transformation run\n",
      "G6043_N4_1745_L5 FGDC record edited\n",
      "18 FGDC records updated\n",
      "\n",
      "G6043_S2_1652_F7 XSLT updated\n",
      "MARC record downloaded from API\n",
      "XSLT transformation run\n",
      "G6043_S2_1652_F7 FGDC record edited\n",
      "19 FGDC records updated\n",
      "\n",
      "G6043_S4_1750_W3 XSLT updated\n",
      "MARC record downloaded from API\n",
      "XSLT transformation run\n",
      "G6043_S4_1750_W3 FGDC record edited\n",
      "20 FGDC records updated\n",
      "\n",
      "G6043_Z7_1635_B5 XSLT updated\n",
      "MARC record downloaded from API\n",
      "XSLT transformation run\n",
      "G6043_Z7_1635_B5 FGDC record edited\n",
      "21 FGDC records updated\n",
      "\n",
      "Metadata creation process complete\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# **MAKE SURE THAT 'MARC21slimUtils' XSLT STYLESHEET IS IN THE 'outputs' FOLDER BEFORE RUNNING THIS SCRIPT!**\n",
    "\n",
    "import csv\n",
    "import lxml.etree as ET\n",
    "import datetime\n",
    "import urllib.request\n",
    "\n",
    "filePath = 'C:/Users/mam466/Desktop/HGL/metadataCreationScript2/outputs/'\n",
    "\n",
    "\n",
    "a = 1\n",
    "\n",
    "#open batch csv file with list of xml files (exported FGDC records from ArcGIS) for editing\n",
    "with open('xml_to_edit.csv') as xmlList:\n",
    "    XMLreader = csv.DictReader(xmlList)\n",
    "    #iterate over list\n",
    "    for line in XMLreader:\n",
    "        \n",
    "         \n",
    "        #open csv file containing target information pulled from Dani's georef spreadsheets (CDP or OnSite csv)\n",
    "        with open('newCDP.csv') as csvFile:\n",
    "            \n",
    "            #parse as a dictionary (OwnerSuppliedName can be used as a key)\n",
    "            CSVreader = csv.DictReader(csvFile)\n",
    "            for row in CSVreader: \n",
    "                #match current XML file to information pulled from georef csv using OwnerSuppliedName as a key\n",
    "                if row['OwnerSuppliedName'] == line['file_name']:\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                     #***** _____update XSLT_____ *****\n",
    "                    #update XSLT file so that it contains current XML file that is being edited\n",
    "                    # Read in the file\n",
    "                    with open('single_sheet_MARC21slim2FGDC_20190104.xsl', 'r') as file :\n",
    "                        filedata = file.read()\n",
    "\n",
    "                        # Replace the target string with current XML file name\n",
    "                        filedata = filedata.replace('rLayerID', line['file_name'])\n",
    "                        #insert current date\n",
    "                        todaysdate = str(datetime.date.today())\n",
    "                        todaysdate = todaysdate.replace('-', '')\n",
    "                        filedata = filedata.replace('xxx_todaysdate_xxx', todaysdate)\n",
    "\n",
    "                    # Write the file out again\n",
    "                    with open(filePath + 'outputXSLT.xsl', 'w') as file:\n",
    "                        file.write(filedata)\n",
    "                    print (line['file_name'] + ' XSLT updated')\n",
    "\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    #***** _____run XSLT_____ *****\n",
    "                   \n",
    "                    f_xsl = filePath + 'outputXSLT.xsl'\n",
    "                    f_xml = filePath + line['file_name'] + '_APIdownload.xml'\n",
    "                    f_out = filePath + line['file_name'] +'_XSLToutput.xml'\n",
    "\n",
    "                    #download MARC record from API\n",
    "                    HOLLIS = (row['HOLLIS'])\n",
    "                    \n",
    "                    if len(HOLLIS) == 9:\n",
    "                        urllib.request.urlretrieve ('http://webservices.lib.harvard.edu/rest/marc/hollis/' + row['HOLLIS'], f_xml)\n",
    "                        print('MARC record downloaded from API')\n",
    "                    elif len(HOLLIS) == 8:\n",
    "                        urllib.request.urlretrieve ('http://webservices.lib.harvard.edu/rest/marc/hollis/0' + row['HOLLIS'], f_xml)\n",
    "                        print('MARC record downloaded from API')\n",
    "                    elif len(HOLLIS) == 7:\n",
    "                        urllib.request.urlretrieve ('http://webservices.lib.harvard.edu/rest/marc/hollis/00' + row['HOLLIS'], f_xml)\n",
    "                        print('MARC record downloaded from API')\n",
    "                    else:\n",
    "                        print ('Error with ' + line['file_name'] + ' HOLLIS number. PLEASE CHECK!')\n",
    "                        print ('The HOLLIS number is ' + len(HOLLIS) + ' numbers long')\n",
    "                    \n",
    "                    dom = ET.parse(f_xml)\n",
    "                    xslt = ET.parse(f_xsl)\n",
    "                    transform = ET.XSLT(xslt)\n",
    "                    newdom = transform(dom)\n",
    "                    newdom.write(f_out, pretty_print=True, xml_declaration=True, encoding='UTF-8')\n",
    "        \n",
    "                    print ('XSLT transformation run')\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                \n",
    "                    #***** _____update FGDC record with target infomration_____ *****\n",
    "                    #open and parse current XML file using ElementTree library\n",
    "                    with open(filePath + line['file_name']+'_XSLToutput.xml', encoding=\"utf8\") as f:\n",
    "                        tree = ET.parse(f)\n",
    "                        root = tree.getroot()\n",
    "\n",
    "                        # index in to: /metadata/idinfo/citation/citeinfo/onlink - 'rLayerID'\n",
    "                        #target1 = root[0][0][0][8]\n",
    "                        # index in to: /metadata/idinfo/descript/abstract – rProjection \n",
    "                        #target2 = root[0][1][0]\n",
    "                        #index in to: /metadata/idinfo/native - xxx_SoftwareVersion_xxx\n",
    "                        #target3 = root[0][9]\n",
    "                        #index in to: /metadata/dataqual/posacc/horizpa/horizpar - rRMS rUnits\n",
    "                        #target4 = root[1][2][0][0]\n",
    "                        #index in to: /metadata/dataqual/lineage/procstep/procdesc – rDataSources rProjection\n",
    "                        #target5 = root[1][3][1][0]\n",
    "\n",
    "\n",
    "                        #target1.text = target1.text.replace('http://hgl.harvard.edu:8080/HGL/jsp/HGL.jsp?action=VColl&VCollName=rLayerID','http://hgl.harvard.edu:8080/HGL/jsp/HGL.jsp?action=VColl&VCollName=' + row['OwnerSuppliedName'])\n",
    "                        #target2.text = target2.text.replace(''This layer is a georeferenced raster image of the historic paper map entitled: Ducatus Chablasius et Lacus Lemanus cum regionibus adjacentibus / It was published by: [Apud Haeredes Ioannes Blaeu], in 1682. Scale [ca. 1:220,000].\\n                    \\n                The image inside the map neatline is georeferenced to the surface of the earth and fit to the rProjection coordinate system. All map collar and inset information is also available as part of the raster image, including any inset maps, profiles, statistical tables, directories, text, illustrations, index maps, legends, or other information associated with the principal map. \\n                    \\n                This map shows features such as drainage, cities and other human settlements, territorial boundaries, shoreline features, and more.  rRelief  Includes also\\n                    \\n                This layer is part of a selection of digitally scanned and georeferenced historic maps from the Harvard Map Collection. These maps typically portray both natural and manmade features. The selection represents a range of originators, ground condition dates, scales, and map purposes.', )\n",
    "                        #target3.text = target3.text.replace('xxx_SoftwareVersion_xxx', row['ArcGIS_version'])\n",
    "                        #target4.text = target4.text.replace\n",
    "                        #target5.text = target5.text.replace\n",
    "\n",
    "                    tree.write(filePath + '/final/' + line['file_name']+'.xml', xml_declaration=True, method='xml', encoding=\"utf8\")    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    #***___manually update target information contained in text blocks in current XML using Python___***\n",
    "                    with open(filePath + '/final/' + line['file_name']+'.xml', 'r', encoding=\"utf8\") as file2 :\n",
    "                        filedata = file2.read()\n",
    "\n",
    "                        filedata = filedata.replace('rLayerID', row['OwnerSuppliedName'])\n",
    "                        \n",
    "                        SoftwareVersion = str(row['ArcGIS_version'])\n",
    "                        if SoftwareVersion.upper().isupper() == False:\n",
    "                            filedata = filedata.replace('xxx_SoftwareVersion_xxx', 'ArcMap ' + row['ArcGIS_version'])\n",
    "                        else:\n",
    "                            filedata = filedata.replace('xxx_SoftwareVersion_xxx', row['ArcGIS_version'])\n",
    "                       \n",
    "                        filedata = filedata.replace('xxx_EPSG_xxx', row['EPSG_code'])\n",
    "                        \n",
    "                        projection = row['Coord_Syst']\n",
    "                        projection = projection.replace('_', ' ')\n",
    "                        filedata = filedata.replace('rProjection', projection)\n",
    "                        \n",
    "                        filedata = filedata.replace('rRMS', row['RMS_error '])\n",
    "                       \n",
    "                        RMS0 = row['RMS_error ']\n",
    "                        if RMS0 == '0':\n",
    "                            filedata = filedata.replace('rUnits', row['units'] + '. The RMS error for this map is listed as 0 because at least 4 control points must be used to calculate an RMS error')\n",
    "                        else:\n",
    "                            filedata = filedata.replace('rUnits', row['units'])\n",
    "                        \n",
    "                        basemap = row['Georeferenced_to..']\n",
    "                        basemap = basemap.replace('_', ' ')\n",
    "                        filedata = filedata.replace('rDataSources', basemap)\n",
    "                        \n",
    "                        rightnow = datetime.datetime.now()\n",
    "                        currentyear = str(rightnow.year)\n",
    "                        currentmonth = str(rightnow.month)\n",
    "                        if len(currentmonth) == 1:\n",
    "                            PubMonth = currentyear + '0' + currentmonth\n",
    "                        else:\n",
    "                            PubMonth = currentyear + currentmonth\n",
    "                        filedata = filedata.replace('xxx_PubMonth_xxx', PubMonth)\n",
    "                        \n",
    "                        #use conditional statement to search for Notes\n",
    "                        if row['notes'] == '':\n",
    "                            filedata = filedata.replace('xxx_GeoRefNote_xxx', '')\n",
    "                        else:\n",
    "                            filedata = filedata.replace('xxx_GeoRefNote_xxx', '\\n\\n\\t\\t\\tGeoreferencing note: '+row['notes']+'\\n\\n\\t\\t\\t')\n",
    "                        \n",
    "                    with open(filePath + '/final/' + line['file_name'] + '.xml', 'w+', encoding=\"utf8\") as file3:\n",
    "                        file3.write(filedata)\n",
    "                    \n",
    "                    print (line['file_name'] + ' FGDC record edited')\n",
    "                    if a == 1:\n",
    "                        print (str(a)+ ' FGDC record updated \\n')\n",
    "                    else:\n",
    "                        print (str(a)+ ' FGDC records updated\\n')\n",
    "                    a = a+1\n",
    "                    \n",
    "xmlList.close()\n",
    "file.close()\n",
    "csvFile.close()\n",
    "f.close()\n",
    "file2.close()\n",
    "file3.close()\n",
    "                    \n",
    "\n",
    "\n",
    "print ('Metadata creation process complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
